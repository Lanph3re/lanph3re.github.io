---
layout: post
cover: 'assets/images/cover8.jpg'
navigation: true
title: Glibc 2.27 Free 함수 정리
date: 2019-05-28
tags: glibc heap malloc pwn
subclass: 'post'
logo: 'assets/images/ghost.png'
author: lanphere
categories: lanphere
disqus: true
---

문제를 풀다가 malloc 소스코드를 공부해야할 일이 생겨서 간단하게 정리했다.

free()를 호출했을 때 내부적으로 호출되는 함수는 _int_free이다.
free가 호출되면 먼저 free의 인자로 들어온 chunk의 size를 구한다.

```
size = chunksize(p);
```

glibc 2.27부터는 tcache라는 기존의 fastbin, smallbin, largebin 등을 사용하기 전에 먼저 사용하는 bin이 새로 생겼다.
그래서 먼저 구한 size를 통해 적절한 tcache가 비어 있는지 확인 후, 비어있다면 tcache에 chunk를 넣고 마무리한다.

```
#if USE_TCACHE
  {
    size_t tc_idx = csize2tidx(size);

    if (tcache && tc_idx < mp_.tcache_bins &&
        tcache->counts[tc_idx] < mp_.tcache_count) {
      tcache_put(p, tc_idx);
      return;
    }
  }
#endif
```

그 다음엔(tcache가 다 찼거나 tcache에 들어갈 수 있는 size의 chunk가 아닌 경우) 해당 chunk가 fastbin에 해당하는지 확인하고,
해당한다면 chunk를 적절한 fastbin에 넣는다.
```
/*
    If eligible, place chunk on a fastbin so it can be found
    and used quickly in malloc.
  */

  if ((unsigned long)(size) <= (unsigned long)(get_max_fast())
#if TRIM_FASTBINS
      /*
        If TRIM_FASTBINS set, don't place chunks
        bordering top into fastbins
      */
      && (chunk_at_offset(p, size) != av->top)
#endif
  ) {

    if (__builtin_expect(
            chunksize_nomask(chunk_at_offset(p, size)) <= 2 * SIZE_SZ, 0) ||
        __builtin_expect(chunksize(chunk_at_offset(p, size)) >= av->system_mem,
                         0)) {
      bool fail = true;
      /* We might not have a lock at this point and concurrent modifications
         of system_mem might result in a false positive.  Redo the test after
         getting the lock.  */
      if (!have_lock) {
        __libc_lock_lock(av->mutex);
        fail = (chunksize_nomask(chunk_at_offset(p, size)) <= 2 * SIZE_SZ ||
                chunksize(chunk_at_offset(p, size)) >= av->system_mem);
        __libc_lock_unlock(av->mutex);
      }

      if (fail) malloc_printerr("free(): invalid next size (fast)");
    }

    free_perturb(chunk2mem(p), size - 2 * SIZE_SZ);

    atomic_store_relaxed(&av->have_fastchunks, true);
    unsigned int idx = fastbin_index(size);
    fb = &fastbin(av, idx);

    /* Atomically link P to its fastbin: P->FD = *FB; *FB = P;  */
    mchunkptr old = *fb, old2;

    if (SINGLE_THREAD_P) {
      /* Check that the top of the bin is not the record we are going to
         add (i.e., double free).  */
      if (__builtin_expect(old == p, 0))
        malloc_printerr("double free or corruption (fasttop)");
      p->fd = old;
      *fb = p;
    } else
      do {
        /* Check that the top of the bin is not the record we are going to
           add (i.e., double free).  */
        if (__builtin_expect(old == p, 0))
          malloc_printerr("double free or corruption (fasttop)");
        p->fd = old2 = old;
      } while ((old = catomic_compare_and_exchange_val_rel(fb, p, old2)) !=
               old2);

    /* Check that size of fastbin chunk at the top is the same as
       size of the chunk that we are adding.  We can dereference OLD
       only if we have the lock, otherwise it might have already been
       allocated again.  */
    if (have_lock && old != NULL &&
        __builtin_expect(fastbin_index(chunksize(old)) != idx, 0))
      malloc_printerr("invalid fastbin entry (free)");
  }

  /*
    Consolidate other non-mmapped chunks as they arrive.
  */
```

그 다음엔 free하려는 chunk가 mmap을 통해 할당된 chunk인지를 확인해서 만약 맞다면 mmap을 통해 할당된 chunk를 해제시켜주는 루틴을 처리한다.

```
/*
  If the chunk was allocated via mmap, release via munmap().
*/
munmap_chunk(p);
```

mmap을 통해 할당된 chunk가 아니라면 몇 가지 검증 루틴을 거친다.
1. free하려는 chunk가 top chunk거나, 
2. 현재 arena에 할당된 메모리 한계를 넘어섰거나, 
3. 다음 chunk의 prev_in_use가 이미 0이면

에러를 발생시킨다.

```
/* Lightweight tests: check whether the block is already the
   top block.  */
if (__glibc_unlikely(p == av->top))
  malloc_printerr("double free or corruption (top)");
/* Or whether the next chunk is beyond the boundaries of the arena.  */
if (__builtin_expect(
        contiguous(av) &&
            (char *)nextchunk >= ((char *)av->top + chunksize(av->top)),
        0))
  malloc_printerr("double free or corruption (out)");
/* Or whether the block is actually not marked used.  */
if (__glibc_unlikely(!prev_inuse(nextchunk)))
  malloc_printerr("double free or corruption (!prev)");
nextsize = chunksize(nextchunk);
if (__builtin_expect(chunksize_nomask(nextchunk) <= 2 * SIZE_SZ, 0) ||
    __builtin_expect(nextsize >= av->system_mem, 0))
  malloc_printerr("free(): invalid next size (normal)");
free_perturb(chunk2mem(p), size - 2 * SIZE_SZ);
```

이렇게 검사 루틴을 거치고나면 
1. free하려는 chunk의 prev_in_use를 확인해서 만약 0이라면 인접한 위에 있는 chunk와 merge한다.
2. 그 다음엔 만약 free하려는 chunk의 다음 chunk가 top chunk라면 그 top chunk와 merge하고,
3. 만약 top chunk가 아니라면 다음 chunk가 free chunk인지 확인하고 맞다면 merge한다.

모든 과정 이후에 최종적인 chunk를 unsorted bin에 넣는다.

```
if (!prev_inuse(p)) {
  prevsize = prev_size(p);
  size += prevsize;
  p = chunk_at_offset(p, -((long)prevsize));
  unlink(av, p, bck, fwd);
}

if (nextchunk != av->top) {
  /* get and clear inuse bit */
  nextinuse = inuse_bit_at_offset(nextchunk, nextsize);
  /* consolidate forward */
  if (!nextinuse) {
    unlink(av, nextchunk, bck, fwd);
    size += nextsize;
  } else
    clear_inuse_bit_at_offset(nextchunk, 0);
  /*
    Place the chunk in unsorted chunk list. Chunks are
    not placed into regular bins until after they have
    been given one chance to be used in malloc.
  */
  bck = unsorted_chunks(av);
  fwd = bck->fd;
  if (__glibc_unlikely(fwd->bk != bck))
    malloc_printerr("free(): corrupted unsorted chunks");
  p->fd = fwd;
  p->bk = bck;
  if (!in_smallbin_range(size)) {
    p->fd_nextsize = NULL;
    p->bk_nextsize = NULL;
  }
  bck->fd = p;
  fwd->bk = p;
  set_head(p, size | PREV_INUSE);
  set_foot(p, size);
  check_free_chunk(av, p);
}
/*
  If the chunk borders the current high end of memory,
  consolidate into top
*/
else {
  size += nextsize;
  set_head(p, size | PREV_INUSE);
  av->top = p;
  check_chunk(av, p);
}
```